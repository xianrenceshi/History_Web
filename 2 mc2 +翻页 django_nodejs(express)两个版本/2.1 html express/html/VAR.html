<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<title>Virtual/Augmented Reality</title>
		<link rel="stylesheet" href="../css/normal.css" />
		<script type="text/javascript" src="../js/head2.js"></script>
	</head>
	<body>
		<div class="banner" style="color:white; text-align: center;font-size: 45px;font-weight: bold;">
			Research
		</div>
		<h1>Virtual/Augmented Reality</h1>
		<div class="mainbody">
			<img src="../img/vr.png" style="text-align: center; margin:20px auto;width:100%;"></img>
			<!--
			<img src="../img/saliency.jpg" style="text-align: center; margin:20px auto;width:100%;"></img>
			<p class="brief">
				 Saliency prediction is a computer vision task, in order to predict the human attention when viewing images or videos. Here, we utilize machine learning methods, such as sparse coding, support vector mechanism and deep natural network to generate pixel-wise saliency map in various situations.
			</p>
			-->
			<h2>Related Publications</h2>
			<hr/>
			<ul class="pubul">
                <li class="content">
					Mai Xu, Chen Li, Zhenzhong Chen, Zulin Wang and Zhenyu Guan, "Assessing Visual Quality of Omnidirectional Videos," in IEEE Transactions on Circuits and Systems for Video Technology (2018).
					<br/>|
					<a href='../pdf/TCSVT2018Assessing.pdf'>Download PDF</a>
					|
					<a href="https://github.com/Archer-Tatsu/head-tracking">Download Database</a>
					|
					<a href = "https://github.com/Archer-Tatsu/CP-PSNR">Download Code</a>
					|
				</li>
				<li class="content">
					Chen Li, Mai Xu, Xinzhe Du, and Zulin Wang. "Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video: A Large-Scale Dataset and a Deep Learning Model." In Proceedings of the 26th ACM International Conference on Multimedia, 2018.
					<br/>|
					<a href='../pdf/MM2018Bridge.pdf'>Download PDF</a>
					|
					<a href = "https://github.com/Archer-Tatsu/VQA-ODV">Download Database</a>
					|
					<a href='../pdf/MM2018Bridge_Poster.pdf'>Download Poster</a>
					|
				</li>
				<li class="content">
					Liu Y, Yang L, Xu M, et al. Rate control schemes for panoramic video coding[J]. Journal of Visual Communication and Image Representation, 2018, 53: 76-85.
					<br/>
				</li>
				<li class="content">
					Mai Xu; Yuhang Song; Jianyi Wang; Minglang Qiao; Liangyu Huo; Zulin Wang, Modeling Attention in Panoramic Video: A Deep Reinforcement Learning Approach, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.
				</li>
				<li class="content">
					Mai Xu, Chen Li, Zulin Wang and Zhenzhong Chen, "Assessing Visual Quality of Omnidirectional Video Coding," IEEE Transactions on Circuits and Systems for Video Technology (In Submitting)
					<br/>
				</li>
				<li class="content">
					Mai Xu, Chen Li, Yufan Liu, Xin Deng, and Jiaxin Lu. "A subjective visual quality assessment method of panoramic videos." In 2017 IEEE International Conference on Multimedia and Expo (ICME), pp. 517-522. IEEE, 2017.
					<br/>|
					<a href='../pdf/ICME2017ASubjective.pdf'>Download PDF</a>
					|
					<a href="https://github.com/Archer-Tatsu/head-tracking">Download Database</a>
					|
					<a href = "https://github.com/Archer-Tatsu/Evaluation_VR-onebar-vive">Download Code</a>
					|
				</li>
				<li class="content">
					Yufan Liu, Mai Xu, Chen Li, Shengxi Li and Zulin Wang. ""A novel rate control scheme for panoramic video coding."" In 2017 IEEE International Conference on Multimedia and Expo (ICME), pp. 691-696. IEEE, 2017.
					<br/>|
					<a href='../pdf/ICME2017ANovelRate.pdf'>Download PDF</a>
					|
				</li>
			</ul>
		</div>
	</body>
	<script type="text/javascript" src="../js/foot2.js"></script>
</html>
