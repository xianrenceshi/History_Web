<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<title>Databases &amp; Codes</title>
		<link rel="stylesheet" href="../css/normal.css" />
		<script type="text/javascript" src="../js/head2.js"></script>
	</head>
	<body>
		<div class="banner" style="color:white; text-align: center;font-size: 45px;font-weight: bold;">
			Databases &amp; Codes
		</div>
		<div class="mainbody">
			<h2>Saliency Detection</h2>
			<hr/>
			<ul>
				<li class="content">
					<h3>RSRCNN</h3>
					Road structure refined CNN (RSRCNN)
					<br/>|
					<a href='https://github.com/yananweinbaa/RSRCNN'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Raw video dataset</h3>
					33 raw videos from the latest HEVC standard test sets, such as JCT-VC, which have been commonly utilized for evaluating HEVC performance.  The eye fixations of all 32 subjects over each video frame were recorded by a Tobii TX300 eye tracker at a sample rate of 300 Hz.
					<br/>|
					<a href='https://github.com/remega/video_database'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Large-scale eye-tracking database of videos (LEDOV)</h3>
					LEDOV includes 538 videos with diverse content, containing a total of 179,336 frames and 6,431 seconds. The diverse content refers to the daily action, sports, social activity and art performance of human, and the videos of animal and man-man objects are also included. Moreover, 32 participants (18 males and 14 females), aging from 20 to 56 (32 on average), were recruited to participate in the eye-tracking experiment.
					<br/>|
					<a href='https://github.com/remega/LEDOV-eye-tracking-database'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Predicting salient face in multiple-face videos</h3>
					Eye tracking data on multiple face videos.
					<br/>|
					<a href='https://github.com/yufanLIU/salient-face-in-MUVFET'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Natural-scene images memorability (NSIM) database</h3>
					NSIM dataset includes 258 natura-scene images and their memorability scores.
					<br/>|
					<a href='https://github.com/Blossomsblue/Memorability'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Face images for saliency detection</h3>
					510 Face images for saliency detection.
					<br/>|
					<a href='https://github.com/RenYun2016/Face'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Face videos for saliency detection</h3>
					Face videos for saliency detection.
					<br/>|
					<a href='https://github.com/RenYun2016/-Particle-Filter-for-Dynamic-GMM'>GitHub</a>
					|
				</li>
			</ul>
			<h2>Video Compression</h2>
			<hr/>
			<ul>
				<li class="content">
					<h3>MFQE 2.0</h3>
					The TensorFlow Code for testing multi-frame quality enhancement (MFQE) approach v2.0.
					<br/>|
					<a href='https://github.com/RyanXingQL/MFQEv2.0'>GitHub</a>
                    |
                    <a href='https://bhpan.buaa.edu.cn/link/0EBF3709E3168E9A78206391258715A3'>北航云盘</a>
                    |
				</li>				
				<li class="content">
					<h3>Database for HEVC in-loop filter</h3>
					A large-scale database for HEVC in-loop filter (HIF).
					<br/>|
					<a href='https://github.com/tianyili2017/HIF-Database'>GitHub</a> |
				</li>
				<li class="content">
					<h3>Subjective-driven complexity control approach for HEVC</h3>
					Subjective-driven Complexity Control Source Code.
					<br/>|
					<a href='https://github.com/cindydeng1991/DENG_TCSVT/'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Eye-tracking database for 40 typical images</h3>
					An eye-tracking database for 40 typical images.
					<br/>|
					<a href='https://drive.google.com/drive/folders/0ByJ_K4vlqqQLTDg4R29QY3F0Qzg'>Google Drive</a>
					|
				</li>
				<li class="content">
					<h3>Database for CU partition of HEVC (CPH)</h3>
					A large-scale database for CU partition of HEVC, to reduce encoding complexity through deep learning based approach.
					<br/>|
					<a href='https://github.com/tianyili2017/CPH'>GitHub (Database)</a>
					|
					<a href='https://github.com/tianyili2017/HEVC-Complexity-Reduction'>GitHub (Code)</a>
					|
				</li>
				<li class="content">
					<h3>DS-CNN</h3>
					The Caffe code of the decoder-side scalable convolutional neural network (DS-CNN)
					<br/>|
					<a href='https://github.com/ryangBUAA/DS-CNN'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>RTE core</h3>
					The core code for the recursive Taylor expansion (RTE) method
					<br/>|
					<a href='https://sites.google.com/site/lsxweb/Home/publication/tcsvt2016source'>Google Sites</a>
					|
				</li>
				<li class="content">
					<h3>Weight-based R-lambda RC</h3>
					The core code for the weight-based R-lambda rate control
					<br/>|
					<a href='https://sites.google.com/site/lsxweb/Home/publication/spic2015source'>Google Sites</a>
					|
				</li>
			</ul>
			<h2>Virtual/Augmented Reality</h2>
			<hr/>
			<ul>
				<li class="content">
					<h3>VQA-ODV</h3>
					A large-scale VQA dataset of omnidirectional video proposed in ACMMM 2018 paper.
					<br/>|
					<a href='https://github.com/Archer-Tatsu/VQA-ODV'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>PVS-HMEM database</h3>
					PVS-HMEM (Panoramic Video Sequences with Head Movement & Eye Movement database) database contains both Head Movement and Eye Movement data of 58 subjects on 76 panoramic videos.
					<br/>|
					<a href='https://github.com/YuhangSong/DHP'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>V-CNN</h3>
					Viewport-based CNN for visual quality assessment on 360° video.
					<br/>|
					<a href = "https://github.com/Archer-Tatsu/V-CNN">GitHub</a>
					|
				</li>
				<li class="content">
					<h3>NCP- and CP-PSNR</h3>
					Perceptual objective quality assessment methods of omnidirectional video.
					<br/>|
					<a href = "https://github.com/Archer-Tatsu/CP-PSNR">GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Panoramic video head tracking data</h3>
					HTC Vive head tracking data of 40 subjects on 48 panoramic video sequences in 8 classes.
					<br/>|
					<a href='https://github.com/Archer-Tatsu/head-tracking'>GitHub</a>
					|
				</li>
				<li class="content">
					<h3>Evaluation_VR</h3>
					A subjective evaluation tool on VR video with GUI in MATLAB. SSCQS and MSCQS method are provided for different aims.
					<br/>|
					<a href='https://github.com/Archer-Tatsu/Evaluation_VR-onebar-vive'>GitHub</a>
					|
				</li>
			</ul>
		</div>
	</body>
	<script type="text/javascript" src="../js/foot2.js"></script>
</html>
